# -*- coding: utf-8 -*-
"""drn.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vx_VY1F0XWRW5cJjuu0ujChGb0kNckDK

**Kartikey Sharma (179301094)**

**Handwritten Digit Recognition**
"""

import tensorflow as tf #pip install tensorflow

mnist = tf.keras.datasets.mnist ## this is a handwritten character based on 28*28 sized images

#unpacking the dataset into train and test datasets
(x_train,y_train),(x_test,y_test)=mnist.load_data()

x_train.shape

import matplotlib.pyplot as plt
plt.imshow(x_train[0])
plt.show()
plt.imshow(x_train[0],cmap=plt.cm.binary)

print(x_train[0]) #before normalization

#normalizing the data | preprocessing step
# you might notice its a grey image and all values varies from 0 to 255 in order to normalize it
x_train =  tf.keras.utils.normalize (x_train,axis=1)
x_test = tf.keras.utils.normalize(x_test,axis=1)
plt.imshow(x_train[0],cmap=plt.cm.binary)

#after normalization
print(x_train[0]) # you can see all values are normalized

print(y_train[0])

## resizing the image to make it suitable for apply convolution operation
import numpy as np #pip install numpy
IMG_SIZE=28
x_trainr=np.array(x_train).reshape(-1,IMG_SIZE,IMG_SIZE,1)##increasing one dimension for kernel operation
x_testr=np.array(x_test).reshape(-1,IMG_SIZE,IMG_SIZE,1)##increasing one dimension for kernel operation
print("Training Sample dimension ",x_trainr.shape)
print("Testing Sample dimension ",x_testr.shape)

#Create deep neural network
### Training on 60000 samples of MNIST Handwritten Dataset

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense,Dropout,Activation,Flatten,Conv2D,MaxPooling2D

mode1 = Sequential()

##first convolution layer 0 1 2 3 (60000,28,28,1) 28-3+1= 26x26
mode1.add(Conv2D(64,(3,3),input_shape = x_trainr.shape[1:]))#only for first convolution layer to mention  input layer size
mode1.add(Activation("relu"))##activation function to make it non-linear,<0,remove,>0
mode1.add(MaxPooling2D(pool_size=(2,2)))##MAxpooling single maximum value of 2*2,

## second convolution layer 26-3+1= 24x24
mode1.add(Conv2D(64,(3,3),input_shape = x_trainr.shape[1:]))#only for first convolution layer to mention  input layer size
mode1.add(Activation("relu"))##activation function to make it non-linear,<0,remove,>0
mode1.add(MaxPooling2D(pool_size=(2,2)))##MAxpooling single maximum value of 2*2,

## third convolution layer
mode1.add(Conv2D(64,(3,3),input_shape = x_trainr.shape[1:]))#only for first convolution layer to mention  input layer size
mode1.add(Activation("relu"))##activation function to make it non-linear,<0,remove,>0
mode1.add(MaxPooling2D(pool_size=(2,2)))##MAxpooling single maximum value of 2*2,

## fully connecting layer 20*20 =400
mode1.add(Flatten())### before using fully connected layer ,need to flatten so that 2D to 1D
mode1.add(Dense(64)) # neural network layer
mode1.add(Activation("relu"))

#Fully Connected layer
mode1.add(Dense(32))
mode1.add(Activation("relu"))

##last fully connected layer ,output must be equal to the number of classes
mode1.add(Dense(10))##last dense layer must be 10
mode1.add(Activation('softmax'))##activation function is changed to softmax

mode1.summary()

print("total training samples",len(x_trainr))

mode1.compile(loss="sparse_categorical_crossentropy", optimizer="adam", metrics=['accuracy'])

training_history=mode1.fit(x_trainr,y_train,epochs=5,validation_split=0.3)##training my model

##evaluating on testing dataset MNIST
test_loss, test_acc = mode1.evaluate(x_testr,y_test)
print("Test loss on 10000 test samples",test_loss)
print("Validation Accuracy on 10000 test samples",test_acc)

plt.xlabel('Epoch Number')
plt.ylabel('Loss')
plt.plot(training_history.history['loss'], label='training set')
plt.plot(training_history.history['val_loss'], label='test set')
plt.legend()

plt.xlabel('Epoch Number')
plt.ylabel('Accuracy')
plt.plot(training_history.history['accuracy'], label='training set')
plt.plot(training_history.history['val_accuracy'], label='test set')
plt.legend()

from matplotlib import pyplot as plt

#predictions = new_model.predict([x_test]) ##there is a specialized method for effeciently saving your model
predictions = mode1.predict([x_testr])

print (predictions)

## in  order to understand ,convert the predictions from one hot encoding , we need to use numpy for that
print(np.argmax(predictions[0]))

### now to check that is our answer is true or not
plt.imshow(x_test[0])

## in order to understand ,convert the predictions from one hot encoding , we need to use numpy for that
print(np.argmax(predictions[110])) ## so actually argmax will return the maximum value index and final value of it

plt.imshow(x_test[110])

import cv2 ## pip install openc-python

import numpy as np

from google.colab import files
from IPython import display

upload = files.upload()

img=cv2.imread('3.png')

plt.imshow(img)

gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)

gray.shape

resized = cv2.resize(gray,(28,28),interpolation = cv2.INTER_AREA)

resized.shape

newimg = tf.keras.utils.normalize (resized,axis=1)

newimg = np.array(newimg).reshape(-1, IMG_SIZE, IMG_SIZE,1)

newimg.shape

predictions = mode1.predict(newimg)

print (np.argmax(predictions))

#Predicting First 10 test images
pred = mode1.predict(x_testr[:10])
# print(pred)
p=np.argmax(pred, axis=1)
print(p)
print(y_test[:10])

#Visualizing prediction
for i in range(10):
  plt.imshow(x_testr[i].reshape((28,28)), cmap='binary')
  plt.title("Original: {}, Predicted: {}".format(y_test[i], p[i]))
  plt.axis("Off")
  plt.figure()

from google.colab import files
from IPython import display

upload = files.upload()

import cv2

upload = files.upload()

iage=cv2.imread('6.png')

plt.imshow(iage)

gray = cv2.cvtColor(iage,cv2.COLOR_BGR2GRAY)

gray.shape

resize = cv2.resize(gray,(28,28),interpolation = cv2.INTER_AREA)

resize.shape

newimage = tf.keras.utils.normalize (resize,axis=1)

newimage = np.array(newimage).reshape(-1, IMG_SIZE, IMG_SIZE,1)

newimage.shape

prediction = mode1.predict(newimage)

print(np.argmax(prediction))

plt.imshow(x_test[81])

import cv2

upload = files.upload()

image1 = cv2.imread('eight.png')

plt.imshow(image1)

gray = cv2.cvtColor(image1,cv2.COLOR_BGR2GRAY)

gray.shape

resize1 = cv2.resize(gray,(28,28),interpolation = cv2.INTER_AREA)

resize1.shape

newimage = tf.keras.utils.normalize (resize1,axis=1)

newimage = np.array(newimage).reshape(-1, IMG_SIZE, IMG_SIZE,1)

newimage.shape

prediction = mode1.predict(newimage)

print(np.argmax(prediction))

upload=files.upload()

img  = cv2.imread('3.png')

plt.imshow(img)

gray  = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)

gray.shape

resized = cv2.resize(gray,(28,28),interpolation = cv2.INTER_AREA)

resized.shape

newimage = tf.keras.utils.normalize (resized,axis=1)

newimage = np.array(newimage).reshape(-1, IMG_SIZE, IMG_SIZE,1)

newimage.shape

predictions = mode1.predict(newimage)

print(np.argmax(predictions))

model_name='digits_recognition_cnn.h5'
mode1.save(model_name,save_format='h5')

loaded_model = tf.keras.models.load_model(model_name)